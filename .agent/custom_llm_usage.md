# ç»Ÿä¸€ LLM é…ç½® - ä½¿ç”¨æŒ‡å—

## âœ… å·²å®Œæˆçš„ä¿®æ”¹

æˆ‘å·²ç»ä¿®æ”¹äº† `cmbagent/utils.py`ï¼Œæ·»åŠ äº†å¯¹è‡ªå®šä¹‰ base_url çš„ LLM çš„æ”¯æŒã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³• 1: ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰

```bash
# è®¾ç½®è‡ªå®šä¹‰ LLM é…ç½®
export CUSTOM_LLM_BASE_URL="http://localhost:8000/v1"
export CUSTOM_LLM_MODEL="llama-3.1-70b"
export CUSTOM_LLM_API_KEY="your-api-key"  # å¯é€‰ï¼ŒæŸäº›æœåŠ¡ä¸éœ€è¦

# è¿è¡Œ cmbagent
cmbagent run
```

### æ–¹æ³• 2: åœ¨ä»£ç ä¸­è®¾ç½®

```python
import os

# åœ¨å¯¼å…¥ cmbagent ä¹‹å‰è®¾ç½®
os.environ["CUSTOM_LLM_BASE_URL"] = "http://localhost:8000/v1"
os.environ["CUSTOM_LLM_MODEL"] = "llama-3.1-70b"
os.environ["CUSTOM_LLM_API_KEY"] = "your-key"

import cmbagent

result = cmbagent.one_shot("Your task here")
```

### æ–¹æ³• 3: ä½¿ç”¨ .env æ–‡ä»¶

åˆ›å»º `.env` æ–‡ä»¶åœ¨é¡¹ç›®æ ¹ç›®å½•ï¼š

```bash
# .env
CUSTOM_LLM_BASE_URL=http://localhost:8000/v1
CUSTOM_LLM_MODEL=llama-3.1-70b
CUSTOM_LLM_API_KEY=your-key
```

ç„¶åä½¿ç”¨ `python-dotenv` åŠ è½½ï¼š

```python
from dotenv import load_dotenv
load_dotenv()

import cmbagent
```

---

## ğŸ“‹ å¸¸è§ LLM æœåŠ¡é…ç½®

### 1. Ollama (æœ¬åœ°)

```bash
export CUSTOM_LLM_BASE_URL="http://localhost:11434/v1"
export CUSTOM_LLM_MODEL="llama3.1:70b"
export CUSTOM_LLM_API_KEY="ollama"
```

**å¯åŠ¨ Ollama æœåŠ¡**:

```bash
ollama serve
ollama pull llama3.1:70b
```

**æµ‹è¯•**:

```bash
curl http://localhost:11434/v1/models
```

---

### 2. vLLM (æœ¬åœ°æˆ–è¿œç¨‹)

```bash
export CUSTOM_LLM_BASE_URL="http://localhost:8000/v1"
export CUSTOM_LLM_MODEL="meta-llama/Llama-3.1-70B-Instruct"
export CUSTOM_LLM_API_KEY="token-abc123"
```

**å¯åŠ¨ vLLM æœåŠ¡**:

```bash
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-3.1-70B-Instruct \
    --served-model-name llama-3.1-70b \
    --api-key token-abc123
```

**æµ‹è¯•**:

```bash
curl http://localhost:8000/v1/models \
    -H "Authorization: Bearer token-abc123"
```

---

### 3. LM Studio (æœ¬åœ°)

```bash
export CUSTOM_LLM_BASE_URL="http://localhost:1234/v1"
export CUSTOM_LLM_MODEL="local-model"
export CUSTOM_LLM_API_KEY="lm-studio"
```

åœ¨ LM Studio ä¸­ï¼š

1. åŠ è½½æ¨¡å‹
2. ç‚¹å‡» "Start Server"
3. ä½¿ç”¨é»˜è®¤ç«¯å£ 1234

**æµ‹è¯•**:

```bash
curl http://localhost:1234/v1/models
```

---

### 4. Together AI

```bash
export CUSTOM_LLM_BASE_URL="https://api.together.xyz/v1"
export CUSTOM_LLM_MODEL="meta-llama/Llama-3-70b-chat-hf"
export CUSTOM_LLM_API_KEY="your-together-api-key"
```

**è·å– API key**: https://api.together.xyz/settings/api-keys

---

### 5. Anyscale

```bash
export CUSTOM_LLM_BASE_URL="https://api.endpoints.anyscale.com/v1"
export CUSTOM_LLM_MODEL="meta-llama/Llama-3-70b-chat-hf"
export CUSTOM_LLM_API_KEY="your-anyscale-api-key"
```

---

### 6. è‡ªå®šä¹‰ OpenAI å…¼å®¹æœåŠ¡

```bash
export CUSTOM_LLM_BASE_URL="https://your-llm-service.com/v1"
export CUSTOM_LLM_MODEL="your-model-name"
export CUSTOM_LLM_API_KEY="your-api-key"
```

---

## ğŸ” éªŒè¯é…ç½®

è¿è¡Œä»¥ä¸‹ Python ä»£ç éªŒè¯é…ç½®:

```python
#!/usr/bin/env python3
import os

# è®¾ç½®é…ç½®
os.environ["CUSTOM_LLM_BASE_URL"] = "http://localhost:8000/v1"
os.environ["CUSTOM_LLM_MODEL"] = "llama-3.1-70b"

# å¯¼å…¥å¹¶æ£€æŸ¥
from cmbagent.utils import (
    default_llm_model,
    default_formatter_model,
    default_llm_config_list,
    CUSTOM_LLM_BASE_URL
)

print("=" * 60)
print("é…ç½®éªŒè¯")
print("=" * 60)

print(f"\nâœ… è‡ªå®šä¹‰ LLM Base URL: {CUSTOM_LLM_BASE_URL}")
print(f"âœ… é»˜è®¤æ¨¡å‹: {default_llm_model}")
print(f"âœ… Formatter æ¨¡å‹: {default_formatter_model}")

print(f"\né»˜è®¤é…ç½®:")
for cfg in default_llm_config_list:
    print(f"  Model: {cfg['model']}")
    print(f"  Base URL: {cfg.get('base_url', 'N/A')}")
    print(f"  API Type: {cfg['api_type']}")

print("\nâœ… é…ç½®æˆåŠŸ!")
```

---

## ğŸ§ª æµ‹è¯•

### ç®€å•æµ‹è¯•

```python
import os
os.environ["CUSTOM_LLM_BASE_URL"] = "http://localhost:8000/v1"
os.environ["CUSTOM_LLM_MODEL"] = "llama-3.1-70b"

import cmbagent

result = cmbagent.one_shot(
    task="Calculate 2 + 2 and explain your answer.",
    agent='engineer'
)

print(result)
```

### æµ‹è¯•æ‰€æœ‰ agents ä½¿ç”¨ç›¸åŒé…ç½®

å½“è®¾ç½® `CUSTOM_LLM_BASE_URL` åï¼Œæ‰€æœ‰ agents ä¼šè‡ªåŠ¨ä½¿ç”¨åŒä¸€ä¸ª LLMï¼š

```python
from cmbagent.utils import default_agent_llm_configs

# æ£€æŸ¥å‰å‡ ä¸ª agents
for agent_name in list(default_agent_llm_configs.keys())[:5]:
    config = default_agent_llm_configs[agent_name]
    print(f"{agent_name}:")
    print(f"  Model: {config['model']}")
    print(f"  Base URL: {config.get('base_url', 'N/A')}")
    print()
```

---

## âš™ï¸ å·¥ä½œåŸç†

### ä¿®æ”¹è¯´æ˜

`cmbagent/utils.py` ä¸­çš„ä¿®æ”¹ï¼š

1. **æ–°å¢ç¯å¢ƒå˜é‡è¯»å–**:

   ```python
   CUSTOM_LLM_BASE_URL = os.getenv("CUSTOM_LLM_BASE_URL")
   CUSTOM_LLM_MODEL = os.getenv("CUSTOM_LLM_MODEL", "llama-3.1-70b")
   CUSTOM_LLM_API_KEY = os.getenv("CUSTOM_LLM_API_KEY", "sk-dummy")
   ```

2. **ä¿®æ”¹ `get_model_config` å‡½æ•°**:
   - æ·»åŠ  `base_url` å‚æ•°
   - ä¼˜å…ˆä½¿ç”¨è‡ªå®šä¹‰é…ç½®
   - ä¿æŒå‘åå…¼å®¹

3. **ä¿®æ”¹é»˜è®¤æ¨¡å‹é…ç½®**:
   - å¦‚æœè®¾ç½®äº† `CUSTOM_LLM_BASE_URL`ï¼Œæ‰€æœ‰æ¨¡å‹ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
   - å¦åˆ™ä½¿ç”¨åŸæœ‰çš„å¤šæ¨¡å‹é…ç½®

4. **ç»Ÿä¸€æ‰€æœ‰ agents**:
   - æ‰€æœ‰ agents çš„ `llm_config` è‡ªåŠ¨æŒ‡å‘è‡ªå®šä¹‰ LLM
   - ç®€åŒ–é…ç½®ç®¡ç†

---

## ğŸ”„ å›é€€åˆ°åŸæœ‰é…ç½®

å¦‚æœéœ€è¦å›é€€ï¼Œåªéœ€**ä¸è®¾ç½®**ç¯å¢ƒå˜é‡ï¼š

```bash
unset CUSTOM_LLM_BASE_URL
unset CUSTOM_LLM_MODEL
unset CUSTOM_LLM_API_KEY
```

æˆ–è€…åœ¨ä»£ç ä¸­ï¼š

```python
import os
os.environ.pop("CUSTOM_LLM_BASE_URL", None)
```

ç³»ç»Ÿä¼šè‡ªåŠ¨ä½¿ç”¨åŸæœ‰çš„å¤š LLM é…ç½®ã€‚

---

## ğŸ“ å®Œæ•´ç¤ºä¾‹

### ä½¿ç”¨ Ollama æœ¬åœ°è¿è¡Œ cmbagent

```bash
#!/bin/bash

# 1. å¯åŠ¨ Ollama
ollama serve &

# 2. ä¸‹è½½æ¨¡å‹
ollama pull llama3.1:70b

# 3. è®¾ç½®ç¯å¢ƒå˜é‡
export CUSTOM_LLM_BASE_URL="http://localhost:11434/v1"
export CUSTOM_LLM_MODEL="llama3.1:70b"
export CUSTOM_LLM_API_KEY="ollama"

# 4. è¿è¡Œ cmbagent
python3 << EOF
import cmbagent

result = cmbagent.one_shot(
    task="Explain what a Large Language Model is in simple terms.",
    agent='engineer'
)

print("Result:")
print(result)
EOF
```

---

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ 1: è¿æ¥å¤±è´¥

```
ConnectionError: HTTPConnectionPool(host='localhost', port=8000)
```

**è§£å†³**:

- ç¡®è®¤ LLM æœåŠ¡æ­£åœ¨è¿è¡Œ
- æ£€æŸ¥ç«¯å£æ˜¯å¦æ­£ç¡®
- æµ‹è¯•è¿æ¥: `curl http://localhost:8000/v1/models`

### é—®é¢˜ 2: è®¤è¯å¤±è´¥

```
AuthenticationError: Invalid API key
```

**è§£å†³**:

- ç¡®è®¤ API key æ­£ç¡®
- æŸäº›æœ¬åœ°æœåŠ¡ä¸éœ€è¦çœŸå® API keyï¼Œä½¿ç”¨å ä½ç¬¦å¦‚ `"sk-dummy"`

### é—®é¢˜ 3: æ¨¡å‹åç§°ä¸åŒ¹é…

```
Model 'llama-3.1-70b' not found
```

**è§£å†³**:

- åˆ—å‡ºå¯ç”¨æ¨¡å‹: `curl http://localhost:8000/v1/models`
- ä½¿ç”¨æœåŠ¡ä¸­å®é™…çš„æ¨¡å‹åç§°

### é—®é¢˜ 4: é…ç½®æœªç”Ÿæ•ˆ

**è§£å†³**:

- ç¡®ä¿åœ¨å¯¼å…¥ cmbagent **ä¹‹å‰**è®¾ç½®ç¯å¢ƒå˜é‡
- é‡æ–°å¯åŠ¨ Python è§£é‡Šå™¨
- æ£€æŸ¥æ‹¼å†™: `CUSTOM_LLM_BASE_URL` (æ³¨æ„ä¸‹åˆ’çº¿)

---

## âœ… ä¼˜åŠ¿æ€»ç»“

- âœ… **ç®€åŒ–é…ç½®**: ç»Ÿä¸€ä½¿ç”¨ä¸€ä¸ª LLMï¼Œæ— éœ€ç®¡ç†å¤šä¸ª API keys
- âœ… **é™ä½æˆæœ¬**: å¯ä½¿ç”¨æœ¬åœ°æˆ–æ›´ä¾¿å®œçš„æœåŠ¡
- âœ… **æé«˜çµæ´»æ€§**: éšæ—¶åˆ‡æ¢ LLM æœåŠ¡
- âœ… **ä¿æŒå…¼å®¹**: ä¸å½±å“åŸæœ‰åŠŸèƒ½ï¼Œå¯éšæ—¶åˆ‡å›
- âœ… **ä¾¿äºæµ‹è¯•**: å¿«é€Ÿæµ‹è¯•ä¸åŒçš„ LLM
- âœ… **æ”¯æŒæœ¬åœ°åŒ–**: å®Œå…¨ç¦»çº¿è¿è¡Œ

---

## ğŸ“š å‚è€ƒèµ„æº

- [Ollama æ–‡æ¡£](https://ollama.ai/)
- [vLLM æ–‡æ¡£](https://docs.vllm.ai/)
- [LM Studio](https://lmstudio.ai/)
- [Together AI](https://www.together.ai/)
- [OpenAI API å…¼å®¹æ€§](https://platform.openai.com/docs/api-reference)

---

## ğŸ’¡ ä¸‹ä¸€æ­¥

1. é€‰æ‹©ä½ çš„ LLM æœåŠ¡
2. è®¾ç½®ç¯å¢ƒå˜é‡
3. è¿è¡Œç®€å•æµ‹è¯•
4. äº«å—ç»Ÿä¸€é…ç½®çš„ä¾¿åˆ©ï¼

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤) éƒ¨åˆ†ã€‚
